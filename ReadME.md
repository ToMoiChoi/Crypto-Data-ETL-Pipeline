# ğŸ§© Crypto Data ETL Pipeline

## ğŸ“˜ Overview

This project demonstrates a complete **ETL (Extract, Transform, Load)**
pipeline built in **Python** for cryptocurrency data. The goal is to
extract real-time market data from the **Binance API**, transform it
into a clean and structured dataset, and load it into a database
(**SQLite** or **BigQuery**) for analytics, automation, and reporting.

This project showcases essential **Data Engineering skills**, including:

-   API integration
-   Data transformation using pandas
-   Database loading
-   Workflow orchestration using **Apache Airflow**
-   Cloud integration (BigQuery)

## âš™ï¸ Tech Stack

  Category                     Tools / Technologies
  ---------------------------- ----------------------------------
  **Programming Language**     Python 3.x
  **Libraries**                pandas, numpy, requests, sqlite3
  **Workflow Orchestration**   Apache Airflow
  **API Source**               Binance REST API
  **Local Database**           SQLite
  **Cloud Data Warehouse**     Google BigQuery
  **Containerization**         Docker / Docker Compose
  **Visualization**            Power BI / matplotlib (optional)
  **Environment**              Jupyter Notebook

## ğŸ§  ETL Workflow

### 1. **Extract**

Retrieve BTC/USDT daily market data from Binance API and convert JSON
into a pandas DataFrame.

``` python
url = "https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1d&limit=30"
response = requests.get(url)
```

### 2. **Transform**

Convert timestamps, clean data, add calculated metrics.

``` python
df['Date'] = pd.to_datetime(df['Open_time'], unit='ms')
df['Daily Change %'] = df['Close'].pct_change() * 100
```

### 3. **Load**

Store data into SQLite or load into BigQuery.

``` python
df.to_sql('CryptoData', con=sql_connection, if_exists='replace', index=False)
```

## ğŸ§© Project Structure

    crypto_etl_pipeline.ipynb     # Notebook version of the ETL pipeline
    crypto_data.csv               # Cleaned crypto dataset
    CryptoData.db                 # SQLite database
    README.md

    airflow/
    â”‚â”€â”€ dags/
    â”‚   â””â”€â”€ crypto_etl_dag.py     # Airflow DAG
    â”‚â”€â”€ logs/                     # Airflow logs
    â”‚â”€â”€ plugins/                  # Optional custom operators
    â”‚â”€â”€ keys/
    â”‚   â””â”€â”€ gcp_key.json          # GCP credentials (gitignored)
    â”‚â”€â”€ docker-compose.yml        # Airflow deployment

# ğŸ”„ Apache Airflow Integration (Workflow Orchestration)

This project includes an **Apache Airflow DAG** that automates the ETL
pipeline.\
Airflow provides:

-   Scheduling
-   Dependency management
-   Logging & monitoring
-   Retry & failure handling

### ETL Flow inside Airflow

    Extract â†’ Transform â†’ Load

### Airflow DAG: `crypto_etl_dag.py`

The DAG orchestrates all 3 tasks:

``` python
extract_task >> transform_task >> load_task
```

## ğŸ‘¤ Author

**TÃ´ Quang Viá»‡t**\
ğŸ“ Hanoi, Vietnam\
ğŸ“§ viettoquang2003@gmail.com\
ğŸ”— https://www.linkedin.com/in/vtqa6\
ğŸ’» https://github.com/viettoqang
